{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as ply\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABRINDO E COLOCANDO DADOS NA FORMATAÇÃO CORRETA (DATAFRAME COM COLUNA AUTOR E COLUNA TEXTO)\n",
    "\n",
    "\n",
    "todos_portais = ['Band','BBC','Folha de SP','Gazeta do Povo','Globo',\n",
    "                 'iG','R7','UOL','Veja','Carta Capital','Revista Forúm',\n",
    "                 'Brasil de Fato','Pleno News','Terça Livre','Renova Mídia',\n",
    "                'Conexão Política','Jornal da Cidade','El Pais', 'Deutsche Welle','Estadão','Isto É']\n",
    "\n",
    "def ler_pickle(nome_portal):\n",
    "    with open('Manchetes/{}.pickle'.format(nome_portal), 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        f.close()\n",
    "    return df\n",
    "\n",
    "def juntando_todas_informacoes(todos_portais):\n",
    "    df_completo = pd.DataFrame(columns=['data','portal','manchete'])\n",
    "    for portal in todos_portais:\n",
    "        df_completo = df_completo.append(ler_pickle(portal))\n",
    "    \n",
    "    return df_completo.reset_index(drop=True)\n",
    "\n",
    "df_textos = juntando_todas_informacoes(todos_portais).drop('data',1)\n",
    "df_textos = df_textos.rename(columns = {'portal': 'Autor', 'manchete': 'Texto'}, inplace = False)\n",
    "\n",
    "# df_textos é uma DataFrame com a primeira coluna sendo o autor e a segundo coluna sendo o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrando_por_autor(df_original, filtro_autor=''):#Se filtro do autor for '' serão considerados todos autores\n",
    "    \n",
    "    # Se o filtro inputado não existir na dataframe, serão considerados todos autores\n",
    "    if filtro_autor not in df_original.Autor.value_counts().index:\n",
    "        filtro_autor=''\n",
    "    \n",
    "    # deixando todos textos do mesmo tamanho\n",
    "    if filtro_autor == '':\n",
    "        nova_df = df_original.copy()\n",
    "    else:\n",
    "        nova_df = df_original.copy()\n",
    "        nova_df = nova_df[nova_df.Autor==filtro_autor]\n",
    "    \n",
    "    return nova_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatando a coluna textos da dataframe\n",
    "\n",
    "def limpar_texto(texto):\n",
    "    \n",
    "    texto = texto.lower() # padroniza as letras\n",
    "    texto = re.sub('(:?https?:\\/\\/[^\\s]*)|www.[^\\s]*', '', texto) # remove os links\n",
    "    texto = re.sub('(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])', '', texto) # remove emojis\n",
    "    #texto = re.sub('[%s]' % re.escape(string.punctuation), ' ', texto) # remove as pontuações\n",
    "    #texto = re.sub('\\w*\\d\\w*', '', texto) # remove as palavras que iniciam com números\n",
    "    \n",
    "    return texto\n",
    "\n",
    "#=============================================================================================================================#\n",
    "\n",
    "def formatando_textos(df_original):\n",
    "    \n",
    "    df_original['Texto'] = df_original.Texto.apply(limpar_texto)\n",
    "    \n",
    "    return df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando cada palavra em um número (token) único\n",
    "\n",
    "def substituindo_palavras_numeros(df_original):\n",
    "\n",
    "    myTokenizer = Tokenizer()\n",
    "    myTokenizer.fit_on_texts(df_original.Texto)\n",
    "\n",
    "    df_nova = df_original[['Autor','Texto']]\n",
    "    df_nova['Texto'] = myTokenizer.texts_to_sequences(df_nova.Texto)\n",
    "    \n",
    "    return df_nova, myTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dataframe com enunciados segmentados\n",
    "    # cada texto vai ser dividio em varios pequenos textos\n",
    "\n",
    "def segmentando_textos(df_original):\n",
    "    \n",
    "    texto_novo = []\n",
    "    autor_novo = []\n",
    "\n",
    "    for i, row in df_original.iterrows():\n",
    "        for j in range(1,len(row.Texto)):\n",
    "            autor_novo.append(row.Autor)\n",
    "            texto_novo.append(row.Texto[:j+1])\n",
    "\n",
    "    df_segmentada = pd.DataFrame({'Autor':autor_novo, 'Texto':texto_novo})\n",
    "    \n",
    "    return df_segmentada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajustes_finais_dataframe(df_original, numero_palavra):\n",
    "    \n",
    "    df_original['Texto'] = pad_sequences(df_original.Texto).tolist()\n",
    "    tamanho_maximo_seq = len(df_original['Texto'][0])\n",
    "    \n",
    "    df_final = pd.DataFrame()\n",
    "    df_final['Predictors'] = df_original['Texto'].apply(lambda x: x[:-1])\n",
    "    df_final['Label'] = df_original['Texto'].apply(lambda x: x[-1])\n",
    "    \n",
    "    nova_label = []\n",
    "    for i,a in df_final.Label.iteritems():\n",
    "        nova_label.append(to_categorical(a, num_classes=numero_palavra))\n",
    "    df_final['Label'] = nova_label\n",
    "    \n",
    "    return df_final, tamanho_maximo_seq\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_modelo(tamanho_maximo_sequencia, total_palavras):\n",
    "    input_tamanho = tamanho_maximo_sequencia - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_palavras, 10, input_length=input_tamanho))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_palavras, activation='softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay = 1e-6)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rodando_programa(df_original,autor=''):\n",
    "    df_filtrada = filtrando_por_autor(df_original,autor)\n",
    "    df_textos_limpos = formatando_textos(df_filtrada)\n",
    "    df_tokenizado, myTokenizer = substituindo_palavras_numeros(df_textos_limpos)\n",
    "    numero_palavras = len(myTokenizer.word_index)+1\n",
    "    df_segmentada = segmentando_textos(df_tokenizado)\n",
    "    df_final, tamanho_sequencia = ajustes_finais_dataframe(df_segmentada, numero_palavras)\n",
    "    #model = criar_modelo(tamanho_sequencia, numero_palavras)\n",
    "    \n",
    "    X = df_final.Predictors\n",
    "    y = df_final.Label\n",
    "    X_corr = np.concatenate(X.apply(lambda x: np.asarray(x))).reshape(X.shape[0],len(X[0]))\n",
    "    y_corr = np.concatenate(y.apply(lambda x: np.asarray(x))).reshape(y.shape[0],len(y[0]))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_corr, y_corr,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 1)\n",
    "    \n",
    "    \n",
    "    return myTokenizer, tamanho_sequencia, numero_palavras, X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytoken, tamanho_maximo, numero_palavras, X_train, X_test, y_train, y_test = rodando_programa(df_textos,'Globo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5646/5646 [==============================] - 90s 16ms/step - loss: 7.7090 - accuracy: 0.0567 - val_loss: 7.6038 - val_accuracy: 0.0526\n",
      "Epoch 2/10\n",
      "5646/5646 [==============================] - 91s 16ms/step - loss: 7.1312 - accuracy: 0.0592 - val_loss: 7.5968 - val_accuracy: 0.0586\n",
      "Epoch 3/10\n",
      "5646/5646 [==============================] - 95s 17ms/step - loss: 6.8214 - accuracy: 0.0698 - val_loss: 7.6281 - val_accuracy: 0.0689\n",
      "Epoch 4/10\n",
      "5646/5646 [==============================] - 100s 18ms/step - loss: 6.5678 - accuracy: 0.0813 - val_loss: 7.6525 - val_accuracy: 0.0737\n",
      "Epoch 5/10\n",
      "5646/5646 [==============================] - 94s 17ms/step - loss: 6.3297 - accuracy: 0.0934 - val_loss: 7.6636 - val_accuracy: 0.0795\n",
      "Epoch 6/10\n",
      "5646/5646 [==============================] - 94s 17ms/step - loss: 6.0856 - accuracy: 0.1047 - val_loss: 7.7132 - val_accuracy: 0.0847\n",
      "Epoch 7/10\n",
      "5646/5646 [==============================] - 97s 17ms/step - loss: 5.8278 - accuracy: 0.1172 - val_loss: 7.7385 - val_accuracy: 0.0890\n",
      "Epoch 8/10\n",
      "5646/5646 [==============================] - 96s 17ms/step - loss: 5.5519 - accuracy: 0.1319 - val_loss: 7.7913 - val_accuracy: 0.0920\n",
      "Epoch 9/10\n",
      "5646/5646 [==============================] - 95s 17ms/step - loss: 5.2847 - accuracy: 0.1475 - val_loss: 7.8433 - val_accuracy: 0.0923\n",
      "Epoch 10/10\n",
      "5646/5646 [==============================] - 96s 17ms/step - loss: 5.0203 - accuracy: 0.1669 - val_loss: 7.8871 - val_accuracy: 0.0944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22f9a51dd88>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "AUTOR = 'Globo'\n",
    "\n",
    "NOME = f'{AUTOR}-AUTOR -- {EPOCHS}-EPOCHS -- {BATCH_SIZE}-BATCH_SIZE -- {int(time.time())}'\n",
    "\n",
    "model = criar_modelo(tamanho_maximo,numero_palavras)\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir=f'logs/{NOME}')\n",
    "\n",
    "#filepath = 'RNN_Final-{epoch:02d}-{val_acc:.3f}'\n",
    "#checkpoint = ModelCheckpoint('models/{}.model'.format(filepath,monitor='val_acc',verbose=1, save_best_only=True, mode='max'))\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 5s 9ms/step - loss: 7.9688 - accuracy: 0.0987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.968751907348633, 0.0986681804060936]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerando_texto(seed_text, next_words, model, max_sequence_len,tokenizer):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        #predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        predict_x=model.predict(token_list)\n",
    "        classes_x=np.argmax(predict_x,axis=1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == classes_x:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bolsonaro É Preso Por Fotos Ao Filho De Viagem De Covid'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gerando_texto('Qualquer Palavra',10,model,tamanho_maximo,mytoken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
